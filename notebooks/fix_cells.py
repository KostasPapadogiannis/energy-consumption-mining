{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fix_categorical_columns",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Î”Î¹ÏŒÏÎ¸Ï‰ÏƒÎ· categorical ÏƒÏ„Î·Î»ÏÎ½ - One Hot Encoding\n",
    "\n",
    "print(\"Î”Î¹ÏŒÏÎ¸Ï‰ÏƒÎ· categorical ÏƒÏ„Î·Î»ÏÎ½ Î³Î¹Î± Î¼Î¿Î½Ï„ÎµÎ»Î¿Ï€Î¿Î¯Î·ÏƒÎ·...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î³Î¹Î± categorical ÏƒÏ„Î®Î»ÎµÏ‚\n",
    "categorical_columns = X_train.select_dtypes(include=['object']).columns\n",
    "print(f\"Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(categorical_columns)} categorical ÏƒÏ„Î®Î»ÎµÏ‚:\")\n",
    "for col in categorical_columns:\n",
    "    print(f\"   - {col}\")\n",
    "    print(f\"     ÎœÎ¿Î½Î±Î´Î¹ÎºÎ­Ï‚ Ï„Î¹Î¼Î­Ï‚: {X_train[col].unique()}\")\n",
    "\n",
    "# One Hot Encoding Î³Î¹Î± categorical ÏƒÏ„Î®Î»ÎµÏ‚\n",
    "X_train_encoded = pd.get_dummies(X_train, columns=categorical_columns, drop_first=True)\n",
    "X_val_encoded = pd.get_dummies(X_val, columns=categorical_columns, drop_first=True)\n",
    "X_test_encoded = pd.get_dummies(X_test, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Î’ÎµÎ²Î±Î¹Ï‰ÏƒÎ· ÏŒÏ„Î¹ ÏŒÎ»Î± Ï„Î± sets Î­Ï‡Î¿Ï…Î½ Ï„Î¹Ï‚ Î¯Î´Î¹ÎµÏ‚ ÏƒÏ„Î®Î»ÎµÏ‚\n",
    "all_columns = set(X_train_encoded.columns) | set(X_val_encoded.columns) | set(X_test_encoded.columns)\n",
    "\n",
    "for col in all_columns:\n",
    "    if col not in X_train_encoded.columns:\n",
    "        X_train_encoded[col] = 0\n",
    "    if col not in X_val_encoded.columns:\n",
    "        X_val_encoded[col] = 0\n",
    "    if col not in X_test_encoded.columns:\n",
    "        X_test_encoded[col] = 0\n",
    "\n",
    "# Î¤Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· ÏƒÏ„Î·Î»ÏÎ½ Î³Î¹Î± consistency\n",
    "X_train_encoded = X_train_encoded[sorted(all_columns)]\n",
    "X_val_encoded = X_val_encoded[sorted(all_columns)]\n",
    "X_test_encoded = X_test_encoded[sorted(all_columns)]\n",
    "\n",
    "print(f\"\\nÎœÎµÏ„Î¬ Ï„Î¿ one-hot encoding:\")\n",
    "print(f\"Train set: {X_train_encoded.shape}\")\n",
    "print(f\"Validation set: {X_val_encoded.shape}\")\n",
    "print(f\"Test set: {X_test_encoded.shape}\")\n",
    "print(f\"ÎÎ­Î± Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬: +{X_train_encoded.shape[1] - X_train.shape[1]}\")\n",
    "\n",
    "# Î‘Î½Ï„Î¹ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· Ï„Ï‰Î½ Ï€Î±Î»Î¹ÏÎ½ Î¼Îµ Ï„Î± Î½Î­Î±\n",
    "X_train = X_train_encoded\n",
    "X_val = X_val_encoded\n",
    "X_test = X_test_encoded\n",
    "\n",
    "print(\"\\nâœ… ÎŸÎ¹ categorical ÏƒÏ„Î®Î»ÎµÏ‚ Î¼ÎµÏ„Î±Ï„ÏÎ¬Ï€Î·ÎºÎ±Î½ Î¼Îµ ÎµÏ€Î¹Ï„Ï…Ï‡Î¯Î±!\")\n",
    "print(\"ğŸš€ Î¤Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± ÎµÎ¯Î½Î±Î¹ Î­Ï„Î¿Î¹Î¼Î± Î³Î¹Î± Î¼Î¿Î½Ï„ÎµÎ»Î¿Ï€Î¿Î¯Î·ÏƒÎ·!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_analysis_fixed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Î“ÏÎ®Î³Î¿ÏÎ· Î±Î½Î¬Î»Ï…ÏƒÎ· Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ (Î´Î¹Î¿ÏÎ¸Ï‰Î¼Î­Î½Î· Î­ÎºÎ´Î¿ÏƒÎ·)\n",
    "\n",
    "print(\"Î‘Î½Î¬Î»Ï…ÏƒÎ· Î§Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Î’Î±ÏƒÎ¹ÎºÎ¬ ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ Î³Î¹Î± Ï„Î± Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬\n",
    "print(f\"\\nÎ£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ Training Î§Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½:\")\n",
    "print(f\"   Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬: {X_train.shape[1]}\")\n",
    "print(f\"   Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ Î´ÎµÎ¯Î³Î¼Î±Ï„Î±: {X_train.shape[0]}\")\n",
    "print(f\"   Î•Î»Î»Î¹Ï€ÎµÎ¯Ï‚ Ï„Î¹Î¼Î­Ï‚: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"   Î¤ÏÏ€Î¿Î¹ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½: {X_train.dtypes.nunique()} Î¼Î¿Î½Î±Î´Î¹ÎºÎ¿Î¯ Ï„ÏÏ€Î¿Î¹\")\n",
    "\n",
    "# ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î³Î¹Î± Î±ÎºÏÎ±Î¯ÎµÏ‚ Ï„Î¹Î¼Î­Ï‚ ÏƒÏ„Î± ÎºÎ±Î½Î¿Î½Î¹ÎºÎ¿Ï€Î¿Î¹Î·Î¼Î­Î½Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±\n",
    "extreme_values = (np.abs(X_train) > 3).sum().sum()\n",
    "print(f\"   Î‘ÎºÏÎ±Î¯ÎµÏ‚ Ï„Î¹Î¼Î­Ï‚ (>3 std): {extreme_values}\")\n",
    "\n",
    "# Top 10 Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ Î¼Îµ Ï„Î· Î¼ÎµÎ³Î±Î»ÏÏ„ÎµÏÎ· Î´Î¹Î±ÎºÏÎ¼Î±Î½ÏƒÎ·\n",
    "feature_variances = X_train.var().sort_values(ascending=False)\n",
    "print(f\"\\nTop 10 Î§Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î· Î”Î¹Î±ÎºÏÎ¼Î±Î½ÏƒÎ·:\")\n",
    "for i, (feature, variance) in enumerate(feature_variances.head(10).items(), 1):\n",
    "    print(f\"   {i:2d}. {feature}: {variance:.4f}\")\n",
    "\n",
    "# ÎŸÏ€Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Î´Î¹Î±ÎºÏÎ¼Î±Î½ÏƒÎ·Ï‚\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_features = feature_variances.head(15)\n",
    "bars = plt.barh(range(len(top_features)), top_features.values)\n",
    "plt.yticks(range(len(top_features)), top_features.index)\n",
    "plt.xlabel('Î”Î¹Î±ÎºÏÎ¼Î±Î½ÏƒÎ·')\n",
    "plt.title('Top 15 Î§Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î· Î”Î¹Î±ÎºÏÎ¼Î±Î½ÏƒÎ·', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· Ï‡ÏÏ‰Î¼Î¬Ï„Ï‰Î½\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(bars)))\n",
    "for bar, color in zip(bars, colors):\n",
    "    bar.set_color(color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nÎ— Î±Î½Î¬Î»Ï…ÏƒÎ· Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ Î¿Î»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ!\")\n",
    "print(\"Î¤Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± ÎµÎ¯Î½Î±Î¹ Î­Ï„Î¿Î¹Î¼Î± Î³Î¹Î± Î¼Î¿Î½Ï„ÎµÎ»Î¿Ï€Î¿Î¯Î·ÏƒÎ· Ï„Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ·Ï‚!\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
